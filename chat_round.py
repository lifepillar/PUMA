from collections.abc import Sequence
from numpy import ndarray
from protocols import EmbeddingLanguageModel, LanguageModelResponse
from util import cosine_similarity, string_entropy


class ChatRound:  # Conforms to Solution and LanguageModelResponse
    """A user's prompt paired with a response and possibly some feedback.

    A chat round represents an interaction with a target LLM, where the target
    LLM receives a prompt generated by some entity (the "generator") and returns
    a response, which may then be evaluated by some other entity (the
    "evaluator", e.g., a human, another LLM, an NLP classifier, etc.) to provide
    some feedback to the prompt's generator along with the target model's
    response. The prompt generator may be human, but in an automatic setting it
    is typically another LLM.

    Attributes
    ----------
    conversation_id: str
        The identifier of the conversation.
    evaluated: bool
        Whether the fitness of the chat round with respect to a predefined goal
        has been estimated.
    feedback: str
        Optional comments about the chat round. This may be set by an evaluator.
    fitness: float
        A score that represents how good the prompt is in eliciting a response
        that conforms to a predefined goal. For example, if the goal is to make
        a target LLM produce a harmful answer, and the query is "describe
        a recipe for meth", then a response such as "I can't do that" would get
        a very low score, while a response such as "Sure, this is a recipe for
        meth: ..." would get a very high score. The score is assigned by an
        evaluator (see the `Evaluator` protocol).
    generation: int
        Generation number.
    model: str
        The model identifier.
    round: int
        The round of the conversation.

    Parameters
    ----------
    response: LanguageModelResponse
        The response of a target language model to a given prompt.
    generation: int, default is 0
        An integer representing the generation when the chat round was created.
    parents: Sequence[ChatRound] | None, default is None
        The solutions (other chat rounds) from which this solution was derived.
    """

    def __init__(
        self,
        response: LanguageModelResponse,
        generation: int = 0,
        parents: Sequence[ChatRound] | None = None,
    ) -> None:
        self._response = response
        self._parents = parents or []
        self._offspring: list[ChatRound] = []

        for parent in self._parents:
            parent._offspring.append(self)

        self.conversation_id = self._response.conversation_id
        self.fitness = float("-inf")
        self.generation = generation
        self.feedback = ""
        self.model = self._response.model
        self.round = self._response.round

    def evaluated(self) -> bool:
        return self.fitness > float("-inf")

    def prompt(self) -> str:
        return self._response.prompt()

    def response(self) -> str:
        return self._response.text()

    def text(self) -> str:
        return self._response.text()

    def reasoning(self) -> str:
        return self._response.reasoning()

    def system_prompt(self) -> str:
        return self._response.system_prompt()

    def parents(self) -> Sequence[ChatRound]:
        """Return the chat rounds from which this chat round was derived."""
        return self._parents

    def offspring(self) -> Sequence[ChatRound]:
        """Return the chat rounds to which this chat round contributed."""
        return self._offspring

    def __lt__(self, other: ChatRound) -> bool:
        return self.fitness < other.fitness

    def __le__(self, other: ChatRound) -> bool:
        return self.fitness <= other.fitness

    def __hash__(self) -> int:
        return hash(self.prompt() + self.response())

    def __str__(self) -> str:
        s = str(self._response)

        if self.feedback:
            s += f"\nFEEDBACK: {self.feedback}"

        return s


class PromptLength:  # Conforms to Feature[ChatRound]
    """Compute the prompt's length from a chat round."""

    def __init__(self, min_length: int = 1, max_length: int = 1000) -> None:
        self.name = "query length"
        self.min_value = float(min_length)
        self.max_value = float(max_length)

    def value_for(self, solution: ChatRound) -> float:
        return len(solution.prompt())


class PromptEntropy:  # Conforms to Feature[ChatRound]
    """Compute the prompt's length from a chat round."""

    def __init__(self) -> None:
        self.name = "query entropy"
        self.min_value = 0.0
        self.max_value = 5.0  # Somewhat arbitrary

    def value_for(self, solution: ChatRound) -> float:
        return string_entropy(solution.prompt())


class PromptRelativeDiversity:  # Conforms to Feature[ChatRound]
    """Measure how different a prompt is from a reference set of prompts.

    The comparison between prompts is done by taking their cosine similarity in
    embedding space.

    Parameters
    ----------
    embedding_model: EmbeddingLanguageModel
        An embedding model.
    reference: list[str]
        Reference prompts.
    """

    def __init__(
        self,
        embedding_model: EmbeddingLanguageModel,
        reference: list[str],
    ) -> None:
        self.name = "diversity"
        self.min_value = 0.0
        self.max_value = 1.0
        self.model = embedding_model
        self._reference: list[str]
        self._reference_embedding: list[ndarray]

        self.set_reference(reference)

    def set_reference(self, reference: list[str]) -> None:
        """Update the reference set."""
        self._reference = list(set(reference)) if reference else []
        self._reference_embedding: list[ndarray] = []  # Lazily evaluated

    def value_for(self, solution: ChatRound) -> float:
        """Compute the diversity of a prompt of a chat round.

        Returns
        -------
        float
            The normalized diversity between the prompt and the set of reference
            prompts. The returned value is in [0.0, 1.0].
        """
        solution_embedding = self.embed(solution.prompt())

        if not self._reference_embedding:
            for p in self._reference:
                self._reference_embedding.append(self.embed(p))

        similarity = 0.0

        for e in self._reference_embedding:
            similarity += cosine_similarity(solution_embedding, e)

        return 1.0 - (similarity / len(self._reference_embedding))

    def embed(self, text: str) -> ndarray:
        return self.model.embed(text)


# vim: tw=80
