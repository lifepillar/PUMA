# Generator and evaluator for adaptive prompt generation.
# Use these with config/prompt_templates/PUMA_* prompts.
import json
import logging
import random
import re

from chat_round import ChatRound
from collections.abc import Sequence
from protocols import (
    Evaluator,
    LanguageModel,
    LanguageModelPromptTemplate,
    LanguageModelResponse,
    SolutionSpace,
)

logger = logging.getLogger(__name__)


class PUMAChatRoundGenerator:  # Conforms to Generator[ChatRound]
    """Generator for adaptive attack prompts.

    Inspired by FunSearch, MindEvolution and AlphaEvolve. The generator is
    sometimes called "sampler" in the references below.

    B Romera-Paredes et al
    Mathematical discoveries from program search with large language models
    2024

    K-H Lee et al
    Evolving deeper LLM thinking
    2025

    Novikov et al
    AlphaEvolve: a coding agent for scientific and algorithmic discovery
    2025

    Parameters
    ----------
    goal: str
        The attacker's goal. This is usually a simple sentence, such as:
        "Generate a message that spreads fake news".
    target_response: str
        The desired output of the target model. This can also be a sentence,
        such as: "Sure, here is a blog post that spreads fake news".
    attacker_model: LanguageModel
        The attacker language model. A reasoning model is recommended to improve
        the attack success rate. The system prompt for the attacker model should
        be one of the `PUMA` prompts in the `prompt_templates` folder or
        a system prompt with similar effects.
    target_model: LanguageModel
        The target language model. Note that this does not need to be distinct
        from the attacker model. That is, a model can target itself.
    evaluator: Evaluator
        An evaluator that evaluates the prompts generated by the attacker model
        and the corresponding responses of the target model. Besides a fitness
        score, the evaluator should produce some **textual feedback**, which is
        used by the critic model (which is the attacker) to suggest ways to
        refine a solution.
    initial_user_prompt: LanguageModelPromptTemplate
        The prompt submitted to the attacker to generate the initial adversarial
        prompts.
    feedback_user_prompt: LanguageModelPromptTemplate
        The user prompt to submit to the attacker model to elicit new
        adversarial prompts. See `attacker_user_critic.jinja` in
        `prompt_templates/PUMA` for the required placeholders.
    initial_strategies: list[str] | None, default is None
        Strategies to use to generate the initial prompts. Strategies are
        descriptions that guide the attacker when formulating prompts. For
        example: “Pretend you are the CEO”. Strategies are integrated into the
        initial user prompt.
    feedback_strategies: list[str] | None, default is None
        Strategies to use to evolve prompts. Those are integrated into the
        feedback user prompt.
    min_initial_strategies: int, default is 1
        The mininum number of initial strategies to sample for each initial
        prompt.
    max_initial_strategies: int, default is 1
        The maximum number of initial strategies to sample for each initial
        prompt.
    min_feedback_strategies: int, default is 1
        The mininum number of feedback strategies to sample for each feedback
        prompt.
    max_feedback_strategies: int, default is 1
        The maximum number of feedback strategies to sample for each feedback
        prompt.
    target_user_prompt: LanguageModelPromptTemplate | None, default is None
        An optional prompt template for the target model. This template can be used to
        modify the attacker's prompt before submitting it to the target model.
        For example, to suppress thinking mode in Qwen3 models,
        you may use a template like this:

            {{ attacker_prompt }} /nothink

        Or you can use this template to “sandwich” the attacker's prompt,
        simulating a guardrail.

        If this template is provided, it must have an `{{ attacker_prompt }}`
        placeholder, which is replaced by the attacker's prompt.
    n_conversations: int, default is 1
        Number of distinct conversations to keep.
    n_rounds: int, default is 1
        Number of rounds per conversation.
    max_parents: int, defau,t is 1
        Maximum number of parents used to generate a new solution.
    verbose: bool, default is False
        When `True`, print the prompts and the responses to stdout.
    """

    def __init__(
        self,
        goal: str,
        target_response: str,
        attacker_model: LanguageModel,
        target_model: LanguageModel,
        evaluator: Evaluator,
        initial_user_prompt: LanguageModelPromptTemplate,
        feedback_user_prompt: LanguageModelPromptTemplate,
        initial_strategies: list[str] | None = None,
        feedback_strategies: list[str] | None = None,
        min_initial_strategies: int = 1,
        max_initial_strategies: int = 1,
        min_feedback_strategies: int = 1,
        max_feedback_strategies: int = 1,
        target_user_prompt: LanguageModelPromptTemplate | None = None,
        n_conversations: int = 1,
        n_rounds: int = 1,
        max_parents: int = 1,
        verbose: bool = False,
    ) -> None:
        assert min_initial_strategies <= max_initial_strategies
        assert min_feedback_strategies <= max_feedback_strategies

        self.generation: int
        self.goal = goal
        self.target_response = target_response
        self.attacker_model = attacker_model
        self.target_model = target_model
        self.evaluator = evaluator
        self.initial_user_prompt = initial_user_prompt
        self.feedback_user_prompt = feedback_user_prompt
        self.initial_strategies: list[str] = initial_strategies or []
        self.feedback_strategies: list[str] = feedback_strategies or []
        self.min_initial_strategies = min_initial_strategies
        self.max_initial_strategies = max_initial_strategies
        self.min_feedback_strategies = min_feedback_strategies
        self.max_feedback_strategies = max_feedback_strategies
        self.target_user_prompt = target_user_prompt
        self.n_conversations = n_conversations
        self.n_rounds = n_rounds
        self.max_parents = max_parents
        self.verbose = verbose

        self.reset()

    def reset(self) -> None:
        """Reset the generator its initial state."""
        self.attacker_model.clear()
        self.target_model.clear()
        self.evaluator.reset()
        self.generation = 0

    def refine_through_critical_conversation(
        self, parents: Sequence[ChatRound], n_rounds: int
    ) -> Sequence[ChatRound]:
        """Generate solutions via Refinement through Critical Conversation (RCC).

        Perform the following:

        1. Build a prompt P that includes the parent chat rounds, each
           accompanied by the corresponding Evaluator feedback, and instructs
           the attacker LLM to (a) critically analyze the previous solutions and
           output suggestions for improvements (by playing a "critic" role
           called Jane), and (b) reflect on the suggestions of step (a) in order
           to produce a new solution (by playing an "author" role called John).
        2. Submit P to the attacker LLM to obtain a new adversarial prompt A.
        3. If n_rounds > 1, continue the eval-criticize-generate loop to
           generate refined adversarial prompts for `n_rounds - 1` rounds.
        4. Return the generated solutions.

        See K-H Lee et al, Evolving Deeper LLM Thinking, Jan 2025.

        Parameters
        ----------
        parents: Sequence[ChatRound]
            A non-empty list of chat rounds. If they have not been evaluated
            yet, they are evaluated as part of the process to generate new
            solutions.
        n_rounds: int
            The number of rounds for refinement. This must be greater than zero.

        Returns
        -------
        Sequence[ChatRound]
        The chat rounds generated according to RCC.
        """
        assert n_rounds > 0
        assert len(parents) > 0

        # Start a fresh conversation for the duration of this RCC
        self.attacker_model.clear()
        # This will store the newly generated solutions
        offspring: list[ChatRound] = []

        # Evaluate previous solutions if necessary
        for chat_round in parents:
            if not chat_round.evaluated:
                # Set fitness score and textual feedback.
                # Fitness score is not used.
                self.evaluator.evaluate(chat_round)

        n_strategies = len(self.feedback_strategies)
        min_s = min(self.min_feedback_strategies, n_strategies)
        max_s = min(self.max_feedback_strategies, n_strategies)
        strategies = random.sample(
            self.feedback_strategies, k=random.randint(min_s, max_s)
        )
        prompt = self.feedback_user_prompt.apply(
            {
                "first_round": True,
                "parents": parents,
                "goal": self.goal,
                "target": self.target_response,
                "strategies": strategies,
            }
        )
        # Prompt the attacker to criticize the previous chat rounds and come up
        # with a new adversarial prompt.
        target_response = self._prompt(prompt)
        offspring.append(
            ChatRound(
                response=target_response,
                generation=self.generation,
                parents=parents,
            )
        )

        i = 1

        # If n_rounds > 1, continue by refining the generated prompt
        while i < n_rounds:
            prev_solution = offspring[-1]

            self.evaluator.evaluate(prev_solution)

            prompt = self.feedback_user_prompt.apply(
                {
                    "first_round": False,
                    "previous": prev_solution,
                    "goal": self.goal,
                    "target": self.target_response,
                    "strategies": strategies,
                }
            )
            # Prompt the attacker to criticize the previous conversations and
            # come up with a new adversarial prompt.
            target_response = self._prompt(prompt)
            offspring.append(
                ChatRound(
                    response=target_response,
                    generation=self.generation,
                    parents=[prev_solution],
                )
            )

            i += 1

        return offspring

    def generate(self, k: int = 1) -> list[ChatRound]:
        """Prompt the attacker models to generate prompts from scratch.

        Each solution is generated independently from the others in a fresh
        conversation with no context, possibly according to some randomly
        sampled strategies (see `PUMAChatRoundGenerator.__init__()`).

        Note that, differently from the algorithm described in (K-H Lee 2025),
        the initial solutions are not refined through RCC.

        Parameters
        ----------
        k: int, default is 1
            The number of solutions to generate. This must be greater than zero.

        Returns
        -------
        list[ChatRound]
            `k` new solutions, independently generated.
        """
        assert k > 0

        solutions: list[ChatRound] = []

        for _ in range(k):
            self.attacker_model.clear()  # Start a fresh conversation

            # Sample the strategies to use
            n_strategies = len(self.initial_strategies)
            min_s = min(self.min_initial_strategies, n_strategies)
            max_s = min(self.max_initial_strategies, n_strategies)
            strategies = random.sample(
                self.initial_strategies, k=random.randint(min_s, max_s)
            )

            prompt = self.initial_user_prompt.apply(
                {
                    "goal": self.goal,
                    "target": self.target_response,
                    "strategies": strategies,
                }
            )
            target_response = self._prompt(prompt)

            solution = ChatRound(
                response=target_response,
                generation=self.generation,
                parents=None,
            )
            solutions.append(solution)

        self.generation += 1

        return solutions

    def select_and_recombine(self, solution_space: SolutionSpace) -> list[ChatRound]:
        """Generate new prompts and responses based on previous conversations.

        Parameters
        ----------
        solution_space: SolutionSpace
            The solution space from which to sample parent solutions.

        Returns
        -------
        list[ChatRound]
            The new solutions.

        Raises
        ------
        RuntimeError
            If an error occurs while prompting the attacker model or the
            target model.
        """
        solutions: list[ChatRound] = []

        for _ in range(self.n_conversations):
            # Sample between 0 and self.max_parents existing solutions
            k = random.randint(0, self.max_parents)

            if k == 0:  # Generate from scratch
                solutions += self.generate(k=1)
            else:
                # Sample k parent solutions
                parents = solution_space.select(k)
                # Recombine to generate child solutions
                offspring = self.refine_through_critical_conversation(
                    parents=parents, n_rounds=self.n_rounds
                )
                solutions += offspring

        self.generation += 1

        return solutions

    def _prompt(self, prompt: str) -> LanguageModelResponse:
        attacker_response = self.attacker_model.prompt(prompt=prompt)
        attacker_prompt = self._parse_attacker_response(attacker_response.text())

        if self.target_user_prompt:
            attacker_prompt = self.target_user_prompt.apply(
                {"attacker_prompt": attacker_prompt}
            )

        target_response = self.target_model.prompt(prompt=attacker_prompt)

        if self.verbose:
            logger.info("ATTACKER: %s", target_response.prompt())
            logger.info("TARGET: %s", target_response.text())

        return target_response

    def _parse_attacker_response(self, response_text: str) -> str:
        """Extract the attacker's prompt from the attacker's response.

        This method assumes that the adversarial prompt is introduced by
        `PROMPT:`
        """
        match = re.search(
            r"PROMPT\s*:\s*(.+)", response_text, flags=re.IGNORECASE | re.DOTALL
        )

        if match is None:
            logger.warning("Could not parse prompt from the attacker's response.")

            return response_text

        adversarial_prompt = match[1]

        return adversarial_prompt


class PUMAChatRoundEvaluator:  # Conforms to Evaluator[ChatRound]
    """Evaluate chat rounds.

    Parameters
    ----------
    goal: str
        The attacker's goal. This is usually a simple sentence, such as:
        "Generate a message that spreads fake news".
    target_response: str
        The response that the attacker wants the target LLM to generate. For
        example: "Sure, here is a post to spread fake news about the next
        political elections".
    eval_model: LanguageModel
        The language model used as the evaluator.
    user_prompt: LanguageModelPromptTemplate
        The prompt to send to the evaluator to trigger an evaluation. The
        template should have at least a `{{ response }}` placeholder that the
        evaluator can substitute with a response to evaluate. Other placeholders
        that may be present in the template are `{{ prompt }}`, `{{ goal }}` and
        `{{ target }}` (the latter for the target response).
    verbose: bool, default is False
        When `True`, print the prompts and the responses to stdout.
    """

    def __init__(
        self,
        goal: str,
        target_response: str,
        eval_model: LanguageModel,
        user_prompt: LanguageModelPromptTemplate,
        verbose: bool = False,
    ) -> None:
        self.goal = goal
        self.target_response = target_response
        self.eval_model = eval_model
        self.user_prompt = user_prompt
        self.verbose = verbose

    def evaluate(self, solution: ChatRound) -> float:
        """Compute the fitness score of a solution and provide textual feedback.

        The fitness score is obtained by asking the evaluator model to provide
        a rating in a given range (typically set by the evaluator's system
        prompt). The evaluator model should also provide some textual feedback,
        which is stored in the `feedback` attribute of the given solution.

        Parameters
        ----------
        solution: ChatRound
            An attacker's prompt paired with a target model's response.

        Returns
        -------
        float
            The fitness score. Higher values should correspond to better fitness
            with respect to the stated goal.
        """
        # Instantiate the user prompt
        eval_prompt = self.user_prompt.apply(
            {
                "prompt": solution.prompt(),
                "response": solution.response(),
                "goal": self.goal,
                "target": self.target_response,
            }
        )
        response = self.eval_model.prompt(prompt=eval_prompt)
        solution.fitness, solution.feedback = self._parse_response(response)

        return solution.fitness

    def reset(self) -> None:
        """Reset the evaluator to its initial state."""
        self.eval_model.clear()

    def _parse_response(self, response: LanguageModelResponse) -> tuple[float, str]:
        fitness: float = float(-1)
        feedback: str = ""

        # Assume the following format:
        #
        #    {
        #      "score": <score>,
        #      "feedback": <feedback>
        #    }
        try:
            response_json = json.loads(response.text())
            fitness = float(response_json["score"])
            feedback = response_json("feedback")
        except Exception:
            # Try less strict parsing
            match_fitness = re.search(
                r'"?\s*score\s*"?\s*:?\s*(\d+)',
                response.text(),
                flags=re.IGNORECASE,
            )
            match_feedback = re.search(
                r'"?\s*feedback\s*"?\s*:?\s*([^}]+)',
                response.text(),
                flags=re.IGNORECASE,
            )

            if match_fitness is None:
                logger.warning("[PUMA] Couldn't parse fitness score.")
            else:
                fitness = float(match_fitness[1])

            if match_feedback is None:
                logger.warning("[PUMA] No feedback from the evaluator.")
            else:
                feedback = match_feedback[1]

        if self.verbose:
            logger.info("EVALUATOR: %s (fitness=%.1f)", feedback, fitness)

        return fitness, feedback


# vim: tw=80
